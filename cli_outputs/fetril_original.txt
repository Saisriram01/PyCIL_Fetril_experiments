2025-03-21 21:43:39,904 [trainer.py] => config: ./exps/fetril.json
2025-03-21 21:43:39,904 [trainer.py] => prefix: train
2025-03-21 21:43:39,904 [trainer.py] => dataset: cifar100
2025-03-21 21:43:39,904 [trainer.py] => memory_size: 0
2025-03-21 21:43:39,904 [trainer.py] => shuffle: True
2025-03-21 21:43:39,904 [trainer.py] => init_cls: 50
2025-03-21 21:43:39,904 [trainer.py] => increment: 10
2025-03-21 21:43:39,904 [trainer.py] => model_name: fetril
2025-03-21 21:43:39,904 [trainer.py] => convnet_type: resnet32
2025-03-21 21:43:39,904 [trainer.py] => device: [device(type='cuda', index=0)]
2025-03-21 21:43:39,905 [trainer.py] => seed: 1993
2025-03-21 21:43:39,905 [trainer.py] => init_epochs: 200
2025-03-21 21:43:39,905 [trainer.py] => actual_epochs: 200
2025-03-21 21:43:39,905 [trainer.py] => init_lr: 0.1
2025-03-21 21:43:39,905 [trainer.py] => init_weight_decay: 0.0005
2025-03-21 21:43:39,905 [trainer.py] => epochs: 50
2025-03-21 21:43:39,905 [trainer.py] => actual: 50
2025-03-21 21:43:39,905 [trainer.py] => lr: 0.05
2025-03-21 21:43:39,905 [trainer.py] => batch_size: 128
2025-03-21 21:43:39,905 [trainer.py] => weight_decay: 0.0005
2025-03-21 21:43:39,905 [trainer.py] => num_workers: 8
2025-03-21 21:43:39,905 [trainer.py] => T: 2
Files already downloaded and verified
Files already downloaded and verified
2025-03-21 21:43:41,929 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2025-03-21 21:43:42,980 [trainer.py] => All params: 464154
2025-03-21 21:43:42,981 [trainer.py] => Trainable params: 464154
Training the model - Task 0
Calling incremental_train
2025-03-21 21:43:42,986 [fetril.py] => Learning on 0-50
2025-03-21 21:43:42,987 [fetril.py] => All params: 467404
2025-03-21 21:43:42,987 [fetril.py] => Trainable params: 467404
network in the _train function IncrementalNet(
  (convnet): CifarResNet(
    (conv_1_3x3): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (stage_1): Sequential(
      (0): ResNetBasicblock(
        (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): ResNetBasicblock(
        (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): ResNetBasicblock(
        (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): ResNetBasicblock(
        (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (4): ResNetBasicblock(
        (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (stage_2): Sequential(
      (0): ResNetBasicblock(
        (conv_a): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): DownsampleA(
          (avg): AvgPool2d(kernel_size=1, stride=2, padding=0)
        )
      )
      (1): ResNetBasicblock(
        (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): ResNetBasicblock(
        (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): ResNetBasicblock(
        (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (4): ResNetBasicblock(
        (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (stage_3): Sequential(
      (0): ResNetBasicblock(
        (conv_a): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): DownsampleA(
          (avg): AvgPool2d(kernel_size=1, stride=2, padding=0)
        )
      )
      (1): ResNetBasicblock(
        (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): ResNetBasicblock(
        (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): ResNetBasicblock(
        (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (4): ResNetBasicblock(
        (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
    (fc): Linear(in_features=64, out_features=10, bias=True)
  )
  (fc): SimpleLinear()
)
2025-03-21 21:43:49,271 [fetril.py] => Task 0, Epoch 1/200 => Loss 3.843, Train_accy 4.99, Test_accy 10.14
2025-03-21 21:43:53,544 [fetril.py] => Task 0, Epoch 2/200 => Loss 3.545, Train_accy 9.37
2025-03-21 21:43:57,778 [fetril.py] => Task 0, Epoch 3/200 => Loss 3.379, Train_accy 12.95
2025-03-21 21:44:01,980 [fetril.py] => Task 0, Epoch 4/200 => Loss 3.233, Train_accy 15.78
2025-03-21 21:44:06,184 [fetril.py] => Task 0, Epoch 5/200 => Loss 3.098, Train_accy 18.74
2025-03-21 21:44:10,959 [fetril.py] => Task 0, Epoch 6/200 => Loss 2.958, Train_accy 21.28, Test_accy 25.62
2025-03-21 21:44:15,322 [fetril.py] => Task 0, Epoch 7/200 => Loss 2.812, Train_accy 24.43
2025-03-21 21:44:19,872 [fetril.py] => Task 0, Epoch 8/200 => Loss 2.680, Train_accy 27.44
2025-03-21 21:44:24,335 [fetril.py] => Task 0, Epoch 9/200 => Loss 2.568, Train_accy 30.16
2025-03-21 21:44:28,537 [fetril.py] => Task 0, Epoch 10/200 => Loss 2.468, Train_accy 31.82
2025-03-21 21:44:33,479 [fetril.py] => Task 0, Epoch 11/200 => Loss 2.383, Train_accy 34.51, Test_accy 38.30
2025-03-21 21:44:37,670 [fetril.py] => Task 0, Epoch 12/200 => Loss 2.309, Train_accy 36.26
2025-03-21 21:44:41,942 [fetril.py] => Task 0, Epoch 13/200 => Loss 2.260, Train_accy 37.48
2025-03-21 21:44:46,119 [fetril.py] => Task 0, Epoch 14/200 => Loss 2.193, Train_accy 39.43
2025-03-21 21:44:50,397 [fetril.py] => Task 0, Epoch 15/200 => Loss 2.167, Train_accy 39.61
2025-03-21 21:44:55,531 [fetril.py] => Task 0, Epoch 16/200 => Loss 2.124, Train_accy 40.71, Test_accy 50.16
2025-03-21 21:44:59,785 [fetril.py] => Task 0, Epoch 17/200 => Loss 2.080, Train_accy 42.05
2025-03-21 21:45:04,015 [fetril.py] => Task 0, Epoch 18/200 => Loss 2.054, Train_accy 42.90
2025-03-21 21:45:08,329 [fetril.py] => Task 0, Epoch 19/200 => Loss 2.038, Train_accy 43.06
2025-03-21 21:45:12,574 [fetril.py] => Task 0, Epoch 20/200 => Loss 2.016, Train_accy 43.90
2025-03-21 21:45:17,500 [fetril.py] => Task 0, Epoch 21/200 => Loss 1.978, Train_accy 44.59, Test_accy 45.68
2025-03-21 21:45:21,746 [fetril.py] => Task 0, Epoch 22/200 => Loss 1.963, Train_accy 44.85
2025-03-21 21:45:25,988 [fetril.py] => Task 0, Epoch 23/200 => Loss 1.960, Train_accy 44.84
2025-03-21 21:45:30,207 [fetril.py] => Task 0, Epoch 24/200 => Loss 1.938, Train_accy 45.82
2025-03-21 21:45:34,456 [fetril.py] => Task 0, Epoch 25/200 => Loss 1.912, Train_accy 46.75
2025-03-21 21:45:39,272 [fetril.py] => Task 0, Epoch 26/200 => Loss 1.896, Train_accy 46.43, Test_accy 51.52
2025-03-21 21:45:43,524 [fetril.py] => Task 0, Epoch 27/200 => Loss 1.886, Train_accy 47.02
2025-03-21 21:45:47,801 [fetril.py] => Task 0, Epoch 28/200 => Loss 1.879, Train_accy 46.98
2025-03-21 21:45:52,051 [fetril.py] => Task 0, Epoch 29/200 => Loss 1.854, Train_accy 47.94
2025-03-21 21:45:56,015 [fetril.py] => Task 0, Epoch 30/200 => Loss 1.859, Train_accy 47.73
2025-03-21 21:46:01,447 [fetril.py] => Task 0, Epoch 31/200 => Loss 1.837, Train_accy 48.44, Test_accy 53.62
2025-03-21 21:46:05,784 [fetril.py] => Task 0, Epoch 32/200 => Loss 1.831, Train_accy 48.53
2025-03-21 21:46:10,035 [fetril.py] => Task 0, Epoch 33/200 => Loss 1.829, Train_accy 48.27
2025-03-21 21:46:14,346 [fetril.py] => Task 0, Epoch 34/200 => Loss 1.837, Train_accy 48.29
2025-03-21 21:46:18,639 [fetril.py] => Task 0, Epoch 35/200 => Loss 1.810, Train_accy 48.59
2025-03-21 21:46:23,532 [fetril.py] => Task 0, Epoch 36/200 => Loss 1.819, Train_accy 48.88, Test_accy 55.98
2025-03-21 21:46:27,740 [fetril.py] => Task 0, Epoch 37/200 => Loss 1.793, Train_accy 49.54
2025-03-21 21:46:31,933 [fetril.py] => Task 0, Epoch 38/200 => Loss 1.797, Train_accy 49.66
2025-03-21 21:46:36,141 [fetril.py] => Task 0, Epoch 39/200 => Loss 1.796, Train_accy 49.57
2025-03-21 21:46:40,340 [fetril.py] => Task 0, Epoch 40/200 => Loss 1.771, Train_accy 50.02
2025-03-21 21:46:45,077 [fetril.py] => Task 0, Epoch 41/200 => Loss 1.772, Train_accy 50.21, Test_accy 54.50
2025-03-21 21:46:49,365 [fetril.py] => Task 0, Epoch 42/200 => Loss 1.747, Train_accy 50.52
2025-03-21 21:46:53,601 [fetril.py] => Task 0, Epoch 43/200 => Loss 1.758, Train_accy 50.56
2025-03-21 21:46:57,794 [fetril.py] => Task 0, Epoch 44/200 => Loss 1.747, Train_accy 51.03
2025-03-21 21:47:02,176 [fetril.py] => Task 0, Epoch 45/200 => Loss 1.750, Train_accy 50.50
2025-03-21 21:47:07,052 [fetril.py] => Task 0, Epoch 46/200 => Loss 1.750, Train_accy 50.38, Test_accy 55.58
2025-03-21 21:47:11,305 [fetril.py] => Task 0, Epoch 47/200 => Loss 1.721, Train_accy 51.12
2025-03-21 21:47:15,535 [fetril.py] => Task 0, Epoch 48/200 => Loss 1.722, Train_accy 51.13
2025-03-21 21:47:19,821 [fetril.py] => Task 0, Epoch 49/200 => Loss 1.716, Train_accy 51.26
2025-03-21 21:47:24,122 [fetril.py] => Task 0, Epoch 50/200 => Loss 1.710, Train_accy 51.46
2025-03-21 21:47:29,369 [fetril.py] => Task 0, Epoch 51/200 => Loss 1.719, Train_accy 51.28, Test_accy 54.22
2025-03-21 21:47:34,173 [fetril.py] => Task 0, Epoch 52/200 => Loss 1.703, Train_accy 51.90
2025-03-21 21:47:38,535 [fetril.py] => Task 0, Epoch 53/200 => Loss 1.703, Train_accy 51.64
2025-03-21 21:47:42,912 [fetril.py] => Task 0, Epoch 54/200 => Loss 1.690, Train_accy 52.13
2025-03-21 21:47:47,242 [fetril.py] => Task 0, Epoch 55/200 => Loss 1.692, Train_accy 52.08
2025-03-21 21:47:52,757 [fetril.py] => Task 0, Epoch 56/200 => Loss 1.680, Train_accy 52.41, Test_accy 53.90
2025-03-21 21:47:57,278 [fetril.py] => Task 0, Epoch 57/200 => Loss 1.677, Train_accy 52.70
2025-03-21 21:48:01,588 [fetril.py] => Task 0, Epoch 58/200 => Loss 1.679, Train_accy 52.39
2025-03-21 21:48:05,995 [fetril.py] => Task 0, Epoch 59/200 => Loss 1.666, Train_accy 52.53
2025-03-21 21:48:10,304 [fetril.py] => Task 0, Epoch 60/200 => Loss 1.666, Train_accy 52.84
2025-03-21 21:48:15,142 [fetril.py] => Task 0, Epoch 61/200 => Loss 1.653, Train_accy 53.09, Test_accy 58.16
2025-03-21 21:48:19,495 [fetril.py] => Task 0, Epoch 62/200 => Loss 1.657, Train_accy 53.06
2025-03-21 21:48:23,822 [fetril.py] => Task 0, Epoch 63/200 => Loss 1.654, Train_accy 52.87
2025-03-21 21:48:28,241 [fetril.py] => Task 0, Epoch 64/200 => Loss 1.640, Train_accy 53.44
2025-03-21 21:48:32,755 [fetril.py] => Task 0, Epoch 65/200 => Loss 1.632, Train_accy 53.64
2025-03-21 21:48:37,812 [fetril.py] => Task 0, Epoch 66/200 => Loss 1.639, Train_accy 53.16, Test_accy 60.58
2025-03-21 21:48:42,117 [fetril.py] => Task 0, Epoch 67/200 => Loss 1.614, Train_accy 54.08
2025-03-21 21:48:46,470 [fetril.py] => Task 0, Epoch 68/200 => Loss 1.626, Train_accy 53.87
2025-03-21 21:48:50,863 [fetril.py] => Task 0, Epoch 69/200 => Loss 1.624, Train_accy 53.90
2025-03-21 21:48:55,253 [fetril.py] => Task 0, Epoch 70/200 => Loss 1.627, Train_accy 54.03
2025-03-21 21:49:00,158 [fetril.py] => Task 0, Epoch 71/200 => Loss 1.606, Train_accy 54.17, Test_accy 57.76
2025-03-21 21:49:04,535 [fetril.py] => Task 0, Epoch 72/200 => Loss 1.590, Train_accy 54.78
2025-03-21 21:49:09,247 [fetril.py] => Task 0, Epoch 73/200 => Loss 1.624, Train_accy 53.79
2025-03-21 21:49:13,600 [fetril.py] => Task 0, Epoch 74/200 => Loss 1.599, Train_accy 54.40
2025-03-21 21:49:17,916 [fetril.py] => Task 0, Epoch 75/200 => Loss 1.592, Train_accy 54.75
2025-03-21 21:49:22,821 [fetril.py] => Task 0, Epoch 76/200 => Loss 1.586, Train_accy 54.75, Test_accy 54.62
2025-03-21 21:49:27,071 [fetril.py] => Task 0, Epoch 77/200 => Loss 1.588, Train_accy 54.89
2025-03-21 21:49:31,380 [fetril.py] => Task 0, Epoch 78/200 => Loss 1.566, Train_accy 55.29
2025-03-21 21:49:35,667 [fetril.py] => Task 0, Epoch 79/200 => Loss 1.558, Train_accy 55.58
2025-03-21 21:49:40,083 [fetril.py] => Task 0, Epoch 80/200 => Loss 1.567, Train_accy 54.81
2025-03-21 21:49:45,406 [fetril.py] => Task 0, Epoch 81/200 => Loss 1.565, Train_accy 55.51, Test_accy 56.52
2025-03-21 21:49:49,883 [fetril.py] => Task 0, Epoch 82/200 => Loss 1.546, Train_accy 55.57
2025-03-21 21:49:54,214 [fetril.py] => Task 0, Epoch 83/200 => Loss 1.563, Train_accy 55.65
2025-03-21 21:49:58,453 [fetril.py] => Task 0, Epoch 84/200 => Loss 1.539, Train_accy 55.94
2025-03-21 21:50:02,729 [fetril.py] => Task 0, Epoch 85/200 => Loss 1.548, Train_accy 55.60
2025-03-21 21:50:07,756 [fetril.py] => Task 0, Epoch 86/200 => Loss 1.550, Train_accy 55.84, Test_accy 62.28
2025-03-21 21:50:12,315 [fetril.py] => Task 0, Epoch 87/200 => Loss 1.532, Train_accy 55.98
2025-03-21 21:50:16,559 [fetril.py] => Task 0, Epoch 88/200 => Loss 1.526, Train_accy 56.23
2025-03-21 21:50:20,836 [fetril.py] => Task 0, Epoch 89/200 => Loss 1.520, Train_accy 56.53
2025-03-21 21:50:25,100 [fetril.py] => Task 0, Epoch 90/200 => Loss 1.509, Train_accy 57.06
2025-03-21 21:50:29,998 [fetril.py] => Task 0, Epoch 91/200 => Loss 1.503, Train_accy 57.26, Test_accy 64.60
2025-03-21 21:50:34,281 [fetril.py] => Task 0, Epoch 92/200 => Loss 1.503, Train_accy 56.65
2025-03-21 21:50:38,527 [fetril.py] => Task 0, Epoch 93/200 => Loss 1.494, Train_accy 57.49
2025-03-21 21:50:42,748 [fetril.py] => Task 0, Epoch 94/200 => Loss 1.485, Train_accy 57.40
2025-03-21 21:50:46,967 [fetril.py] => Task 0, Epoch 95/200 => Loss 1.476, Train_accy 57.70
2025-03-21 21:50:51,744 [fetril.py] => Task 0, Epoch 96/200 => Loss 1.482, Train_accy 57.50, Test_accy 58.12
2025-03-21 21:50:55,952 [fetril.py] => Task 0, Epoch 97/200 => Loss 1.460, Train_accy 58.43
2025-03-21 21:51:00,127 [fetril.py] => Task 0, Epoch 98/200 => Loss 1.471, Train_accy 57.84
2025-03-21 21:51:04,425 [fetril.py] => Task 0, Epoch 99/200 => Loss 1.463, Train_accy 57.93
2025-03-21 21:51:08,634 [fetril.py] => Task 0, Epoch 100/200 => Loss 1.461, Train_accy 57.92
2025-03-21 21:51:13,426 [fetril.py] => Task 0, Epoch 101/200 => Loss 1.471, Train_accy 57.98, Test_accy 64.66
2025-03-21 21:51:17,674 [fetril.py] => Task 0, Epoch 102/200 => Loss 1.436, Train_accy 58.62
2025-03-21 21:51:22,018 [fetril.py] => Task 0, Epoch 103/200 => Loss 1.436, Train_accy 59.05
2025-03-21 21:51:26,240 [fetril.py] => Task 0, Epoch 104/200 => Loss 1.437, Train_accy 58.64
2025-03-21 21:51:30,491 [fetril.py] => Task 0, Epoch 105/200 => Loss 1.412, Train_accy 59.27
2025-03-21 21:51:35,231 [fetril.py] => Task 0, Epoch 106/200 => Loss 1.431, Train_accy 58.60, Test_accy 58.82
2025-03-21 21:51:39,475 [fetril.py] => Task 0, Epoch 107/200 => Loss 1.405, Train_accy 59.61
2025-03-21 21:51:43,698 [fetril.py] => Task 0, Epoch 108/200 => Loss 1.406, Train_accy 59.33
2025-03-21 21:51:47,928 [fetril.py] => Task 0, Epoch 109/200 => Loss 1.405, Train_accy 59.42
2025-03-21 21:51:52,166 [fetril.py] => Task 0, Epoch 110/200 => Loss 1.405, Train_accy 59.71
2025-03-21 21:51:56,963 [fetril.py] => Task 0, Epoch 111/200 => Loss 1.402, Train_accy 59.90, Test_accy 68.08
2025-03-21 21:52:01,244 [fetril.py] => Task 0, Epoch 112/200 => Loss 1.383, Train_accy 60.12
2025-03-21 21:52:05,445 [fetril.py] => Task 0, Epoch 113/200 => Loss 1.376, Train_accy 60.11
2025-03-21 21:52:09,703 [fetril.py] => Task 0, Epoch 114/200 => Loss 1.370, Train_accy 60.88
2025-03-21 21:52:13,905 [fetril.py] => Task 0, Epoch 115/200 => Loss 1.369, Train_accy 60.49
2025-03-21 21:52:18,739 [fetril.py] => Task 0, Epoch 116/200 => Loss 1.352, Train_accy 60.99, Test_accy 62.98
2025-03-21 21:52:22,967 [fetril.py] => Task 0, Epoch 117/200 => Loss 1.344, Train_accy 60.95
2025-03-21 21:52:27,204 [fetril.py] => Task 0, Epoch 118/200 => Loss 1.336, Train_accy 61.52
2025-03-21 21:52:31,486 [fetril.py] => Task 0, Epoch 119/200 => Loss 1.339, Train_accy 61.42
2025-03-21 21:52:35,735 [fetril.py] => Task 0, Epoch 120/200 => Loss 1.327, Train_accy 61.56
2025-03-21 21:52:40,548 [fetril.py] => Task 0, Epoch 121/200 => Loss 1.323, Train_accy 61.84, Test_accy 68.88
2025-03-21 21:52:44,762 [fetril.py] => Task 0, Epoch 122/200 => Loss 1.311, Train_accy 62.35
2025-03-21 21:52:49,023 [fetril.py] => Task 0, Epoch 123/200 => Loss 1.304, Train_accy 62.28
2025-03-21 21:52:53,208 [fetril.py] => Task 0, Epoch 124/200 => Loss 1.321, Train_accy 61.67
2025-03-21 21:52:57,460 [fetril.py] => Task 0, Epoch 125/200 => Loss 1.295, Train_accy 62.32
2025-03-21 21:53:02,290 [fetril.py] => Task 0, Epoch 126/200 => Loss 1.295, Train_accy 62.88, Test_accy 69.68
2025-03-21 21:53:06,553 [fetril.py] => Task 0, Epoch 127/200 => Loss 1.285, Train_accy 62.60
2025-03-21 21:53:10,770 [fetril.py] => Task 0, Epoch 128/200 => Loss 1.265, Train_accy 63.43
2025-03-21 21:53:15,039 [fetril.py] => Task 0, Epoch 129/200 => Loss 1.277, Train_accy 63.26
2025-03-21 21:53:19,315 [fetril.py] => Task 0, Epoch 130/200 => Loss 1.267, Train_accy 63.39
2025-03-21 21:53:24,077 [fetril.py] => Task 0, Epoch 131/200 => Loss 1.253, Train_accy 63.84, Test_accy 70.44
2025-03-21 21:53:28,354 [fetril.py] => Task 0, Epoch 132/200 => Loss 1.237, Train_accy 64.04
2025-03-21 21:53:32,551 [fetril.py] => Task 0, Epoch 133/200 => Loss 1.234, Train_accy 64.37
2025-03-21 21:53:36,784 [fetril.py] => Task 0, Epoch 134/200 => Loss 1.240, Train_accy 63.92
2025-03-21 21:53:41,019 [fetril.py] => Task 0, Epoch 135/200 => Loss 1.220, Train_accy 64.73
2025-03-21 21:53:45,807 [fetril.py] => Task 0, Epoch 136/200 => Loss 1.224, Train_accy 64.48, Test_accy 69.80
2025-03-21 21:53:50,014 [fetril.py] => Task 0, Epoch 137/200 => Loss 1.205, Train_accy 64.76
2025-03-21 21:53:54,283 [fetril.py] => Task 0, Epoch 138/200 => Loss 1.200, Train_accy 65.18
2025-03-21 21:53:58,492 [fetril.py] => Task 0, Epoch 139/200 => Loss 1.182, Train_accy 65.76
2025-03-21 21:54:02,781 [fetril.py] => Task 0, Epoch 140/200 => Loss 1.170, Train_accy 66.17
2025-03-21 21:54:07,555 [fetril.py] => Task 0, Epoch 141/200 => Loss 1.163, Train_accy 66.04, Test_accy 72.02
2025-03-21 21:54:11,774 [fetril.py] => Task 0, Epoch 142/200 => Loss 1.167, Train_accy 66.30
2025-03-21 21:54:15,954 [fetril.py] => Task 0, Epoch 143/200 => Loss 1.153, Train_accy 66.15
2025-03-21 21:54:19,997 [fetril.py] => Task 0, Epoch 144/200 => Loss 1.148, Train_accy 66.43
2025-03-21 21:54:24,314 [fetril.py] => Task 0, Epoch 145/200 => Loss 1.134, Train_accy 67.02
2025-03-21 21:54:28,745 [fetril.py] => Task 0, Epoch 146/200 => Loss 1.126, Train_accy 66.97, Test_accy 73.40
2025-03-21 21:54:33,058 [fetril.py] => Task 0, Epoch 147/200 => Loss 1.101, Train_accy 67.86
2025-03-21 21:54:37,282 [fetril.py] => Task 0, Epoch 148/200 => Loss 1.126, Train_accy 67.19
2025-03-21 21:54:41,540 [fetril.py] => Task 0, Epoch 149/200 => Loss 1.108, Train_accy 67.78
2025-03-21 21:54:45,759 [fetril.py] => Task 0, Epoch 150/200 => Loss 1.103, Train_accy 67.83
2025-03-21 21:54:50,582 [fetril.py] => Task 0, Epoch 151/200 => Loss 1.083, Train_accy 68.53, Test_accy 74.32
2025-03-21 21:54:54,845 [fetril.py] => Task 0, Epoch 152/200 => Loss 1.070, Train_accy 68.59
2025-03-21 21:54:59,113 [fetril.py] => Task 0, Epoch 153/200 => Loss 1.075, Train_accy 68.54
2025-03-21 21:55:03,329 [fetril.py] => Task 0, Epoch 154/200 => Loss 1.062, Train_accy 69.14
2025-03-21 21:55:07,570 [fetril.py] => Task 0, Epoch 155/200 => Loss 1.050, Train_accy 69.11
2025-03-21 21:55:12,344 [fetril.py] => Task 0, Epoch 156/200 => Loss 1.044, Train_accy 69.57, Test_accy 75.22
2025-03-21 21:55:16,580 [fetril.py] => Task 0, Epoch 157/200 => Loss 1.032, Train_accy 69.71
2025-03-21 21:55:20,846 [fetril.py] => Task 0, Epoch 158/200 => Loss 1.034, Train_accy 69.88
2025-03-21 21:55:25,060 [fetril.py] => Task 0, Epoch 159/200 => Loss 1.019, Train_accy 69.82
2025-03-21 21:55:29,355 [fetril.py] => Task 0, Epoch 160/200 => Loss 1.015, Train_accy 70.38
2025-03-21 21:55:34,129 [fetril.py] => Task 0, Epoch 161/200 => Loss 0.992, Train_accy 70.82, Test_accy 76.32
2025-03-21 21:55:38,409 [fetril.py] => Task 0, Epoch 162/200 => Loss 0.992, Train_accy 71.17
2025-03-21 21:55:42,635 [fetril.py] => Task 0, Epoch 163/200 => Loss 0.977, Train_accy 71.48
2025-03-21 21:55:46,879 [fetril.py] => Task 0, Epoch 164/200 => Loss 0.964, Train_accy 71.77
2025-03-21 21:55:51,126 [fetril.py] => Task 0, Epoch 165/200 => Loss 0.957, Train_accy 72.07
2025-03-21 21:55:55,916 [fetril.py] => Task 0, Epoch 166/200 => Loss 0.947, Train_accy 72.21, Test_accy 77.56
2025-03-21 21:56:00,225 [fetril.py] => Task 0, Epoch 167/200 => Loss 0.926, Train_accy 73.12
2025-03-21 21:56:04,480 [fetril.py] => Task 0, Epoch 168/200 => Loss 0.934, Train_accy 72.61
2025-03-21 21:56:08,701 [fetril.py] => Task 0, Epoch 169/200 => Loss 0.918, Train_accy 73.00
2025-03-21 21:56:13,007 [fetril.py] => Task 0, Epoch 170/200 => Loss 0.904, Train_accy 73.54
2025-03-21 21:56:17,850 [fetril.py] => Task 0, Epoch 171/200 => Loss 0.890, Train_accy 73.64, Test_accy 78.54
2025-03-21 21:56:22,082 [fetril.py] => Task 0, Epoch 172/200 => Loss 0.885, Train_accy 73.93
2025-03-21 21:56:26,472 [fetril.py] => Task 0, Epoch 173/200 => Loss 0.884, Train_accy 74.20
2025-03-21 21:56:30,801 [fetril.py] => Task 0, Epoch 174/200 => Loss 0.869, Train_accy 74.49
2025-03-21 21:56:35,112 [fetril.py] => Task 0, Epoch 175/200 => Loss 0.861, Train_accy 74.86
2025-03-21 21:56:40,016 [fetril.py] => Task 0, Epoch 176/200 => Loss 0.844, Train_accy 74.99, Test_accy 78.88
2025-03-21 21:56:44,281 [fetril.py] => Task 0, Epoch 177/200 => Loss 0.860, Train_accy 74.44
2025-03-21 21:56:48,479 [fetril.py] => Task 0, Epoch 178/200 => Loss 0.860, Train_accy 74.78
2025-03-21 21:56:52,731 [fetril.py] => Task 0, Epoch 179/200 => Loss 0.837, Train_accy 75.28
2025-03-21 21:56:56,983 [fetril.py] => Task 0, Epoch 180/200 => Loss 0.832, Train_accy 75.36
2025-03-21 21:57:01,764 [fetril.py] => Task 0, Epoch 181/200 => Loss 0.823, Train_accy 75.71, Test_accy 80.02
2025-03-21 21:57:05,977 [fetril.py] => Task 0, Epoch 182/200 => Loss 0.814, Train_accy 75.82
2025-03-21 21:57:10,259 [fetril.py] => Task 0, Epoch 183/200 => Loss 0.805, Train_accy 76.38
2025-03-21 21:57:14,523 [fetril.py] => Task 0, Epoch 184/200 => Loss 0.806, Train_accy 76.44
2025-03-21 21:57:18,826 [fetril.py] => Task 0, Epoch 185/200 => Loss 0.794, Train_accy 76.72
2025-03-21 21:57:23,592 [fetril.py] => Task 0, Epoch 186/200 => Loss 0.792, Train_accy 76.67, Test_accy 80.48
2025-03-21 21:57:27,875 [fetril.py] => Task 0, Epoch 187/200 => Loss 0.781, Train_accy 77.20
2025-03-21 21:57:32,171 [fetril.py] => Task 0, Epoch 188/200 => Loss 0.771, Train_accy 77.30
2025-03-21 21:57:36,353 [fetril.py] => Task 0, Epoch 189/200 => Loss 0.778, Train_accy 76.98
2025-03-21 21:57:40,622 [fetril.py] => Task 0, Epoch 190/200 => Loss 0.778, Train_accy 77.10
2025-03-21 21:57:45,405 [fetril.py] => Task 0, Epoch 191/200 => Loss 0.765, Train_accy 77.40, Test_accy 80.18
2025-03-21 21:57:49,784 [fetril.py] => Task 0, Epoch 192/200 => Loss 0.759, Train_accy 77.95
2025-03-21 21:57:54,055 [fetril.py] => Task 0, Epoch 193/200 => Loss 0.770, Train_accy 77.39
2025-03-21 21:57:58,382 [fetril.py] => Task 0, Epoch 194/200 => Loss 0.744, Train_accy 78.37
2025-03-21 21:58:02,621 [fetril.py] => Task 0, Epoch 195/200 => Loss 0.754, Train_accy 77.86
2025-03-21 21:58:07,413 [fetril.py] => Task 0, Epoch 196/200 => Loss 0.757, Train_accy 77.81, Test_accy 80.44
2025-03-21 21:58:11,685 [fetril.py] => Task 0, Epoch 197/200 => Loss 0.762, Train_accy 77.70
2025-03-21 21:58:16,018 [fetril.py] => Task 0, Epoch 198/200 => Loss 0.758, Train_accy 78.04
2025-03-21 21:58:20,268 [fetril.py] => Task 0, Epoch 199/200 => Loss 0.742, Train_accy 78.52
2025-03-21 21:58:24,494 [fetril.py] => Task 0, Epoch 200/200 => Loss 0.745, Train_accy 78.08

2025-03-21 21:58:56,604 [fetril.py] => svm train: acc: 94.85
2025-03-21 21:58:56,636 [fetril.py] => svm evaluation: acc_list: [80.42]
Evaluating the model
Calling After Task
2025-03-21 21:58:57,262 [trainer.py] => No NME accuracy.
2025-03-21 21:58:57,262 [trainer.py] => CNN: {'total': 80.48, '00-09': 84.7, '10-19': 74.4, '20-29': 83.1, '30-39': 78.9, '40-49': 81.3, 'old': 0, 'new': 80.48}
2025-03-21 21:58:57,263 [trainer.py] => CNN top1 curve: [80.48]
2025-03-21 21:58:57,263 [trainer.py] => CNN top5 curve: [96.58]

Average Accuracy (CNN): 80.48
2025-03-21 21:58:57,263 [trainer.py] => Average Accuracy (CNN): 80.48
2025-03-21 21:58:57,263 [trainer.py] => All params: 467404
2025-03-21 21:58:57,263 [trainer.py] => Trainable params: 467404
Training the model - Task 1
Calling incremental_train
2025-03-21 21:58:57,272 [fetril.py] => Learning on 50-60
2025-03-21 21:58:57,273 [fetril.py] => All params: 468054
2025-03-21 21:58:57,273 [fetril.py] => Trainable params: 3900
network in the _train function IncrementalNet(
  (convnet): CifarResNet(
    (conv_1_3x3): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (stage_1): Sequential(
      (0): ResNetBasicblock(
        (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): ResNetBasicblock(
        (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): ResNetBasicblock(
        (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): ResNetBasicblock(
        (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (4): ResNetBasicblock(
        (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (stage_2): Sequential(
      (0): ResNetBasicblock(
        (conv_a): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): DownsampleA(
          (avg): AvgPool2d(kernel_size=1, stride=2, padding=0)
        )
      )
      (1): ResNetBasicblock(
        (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): ResNetBasicblock(
        (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): ResNetBasicblock(
        (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (4): ResNetBasicblock(
        (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (stage_3): Sequential(
      (0): ResNetBasicblock(
        (conv_a): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): DownsampleA(
          (avg): AvgPool2d(kernel_size=1, stride=2, padding=0)
        )
      )
      (1): ResNetBasicblock(
        (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): ResNetBasicblock(
        (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): ResNetBasicblock(
        (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (4): ResNetBasicblock(
        (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
    (fc): Linear(in_features=64, out_features=10, bias=True)
  )
  (fc): SimpleLinear()
)
2025-03-21 21:59:12,530 [fetril.py] => Task 1, Epoch 1/50 => Loss 0.515, Train_accy 85.33, Test_accy 71.03
2025-03-21 21:59:14,144 [fetril.py] => Task 1, Epoch 2/50 => Loss 0.373, Train_accy 88.50
2025-03-21 21:59:15,699 [fetril.py] => Task 1, Epoch 3/50 => Loss 0.350, Train_accy 89.33
2025-03-21 21:59:17,244 [fetril.py] => Task 1, Epoch 4/50 => Loss 0.339, Train_accy 89.68
2025-03-21 21:59:18,850 [fetril.py] => Task 1, Epoch 5/50 => Loss 0.330, Train_accy 90.03
2025-03-21 21:59:21,017 [fetril.py] => Task 1, Epoch 6/50 => Loss 0.325, Train_accy 90.12, Test_accy 71.22
2025-03-21 21:59:22,586 [fetril.py] => Task 1, Epoch 7/50 => Loss 0.321, Train_accy 90.25
2025-03-21 21:59:24,176 [fetril.py] => Task 1, Epoch 8/50 => Loss 0.318, Train_accy 90.30
2025-03-21 21:59:25,730 [fetril.py] => Task 1, Epoch 9/50 => Loss 0.314, Train_accy 90.46
2025-03-21 21:59:27,315 [fetril.py] => Task 1, Epoch 10/50 => Loss 0.312, Train_accy 90.61
2025-03-21 21:59:29,589 [fetril.py] => Task 1, Epoch 11/50 => Loss 0.310, Train_accy 90.60, Test_accy 70.33
2025-03-21 21:59:31,212 [fetril.py] => Task 1, Epoch 12/50 => Loss 0.307, Train_accy 90.79
2025-03-21 21:59:32,840 [fetril.py] => Task 1, Epoch 13/50 => Loss 0.306, Train_accy 90.86
2025-03-21 21:59:34,383 [fetril.py] => Task 1, Epoch 14/50 => Loss 0.303, Train_accy 90.84
2025-03-21 21:59:35,919 [fetril.py] => Task 1, Epoch 15/50 => Loss 0.303, Train_accy 90.94
2025-03-21 21:59:38,117 [fetril.py] => Task 1, Epoch 16/50 => Loss 0.301, Train_accy 91.02, Test_accy 70.88
2025-03-21 21:59:39,777 [fetril.py] => Task 1, Epoch 17/50 => Loss 0.300, Train_accy 91.10
2025-03-21 21:59:41,451 [fetril.py] => Task 1, Epoch 18/50 => Loss 0.299, Train_accy 91.03
2025-03-21 21:59:42,941 [fetril.py] => Task 1, Epoch 19/50 => Loss 0.299, Train_accy 91.23
2025-03-21 21:59:44,531 [fetril.py] => Task 1, Epoch 20/50 => Loss 0.298, Train_accy 91.00
2025-03-21 21:59:46,760 [fetril.py] => Task 1, Epoch 21/50 => Loss 0.297, Train_accy 91.18, Test_accy 70.57
2025-03-21 21:59:48,395 [fetril.py] => Task 1, Epoch 22/50 => Loss 0.295, Train_accy 91.24
2025-03-21 21:59:49,936 [fetril.py] => Task 1, Epoch 23/50 => Loss 0.295, Train_accy 91.24
2025-03-21 21:59:51,552 [fetril.py] => Task 1, Epoch 24/50 => Loss 0.293, Train_accy 91.39
2025-03-21 21:59:53,110 [fetril.py] => Task 1, Epoch 25/50 => Loss 0.293, Train_accy 91.31
2025-03-21 21:59:55,324 [fetril.py] => Task 1, Epoch 26/50 => Loss 0.291, Train_accy 91.36, Test_accy 71.02
2025-03-21 21:59:56,927 [fetril.py] => Task 1, Epoch 27/50 => Loss 0.291, Train_accy 91.33
2025-03-21 21:59:58,522 [fetril.py] => Task 1, Epoch 28/50 => Loss 0.291, Train_accy 91.42
2025-03-21 22:00:00,049 [fetril.py] => Task 1, Epoch 29/50 => Loss 0.289, Train_accy 91.40
2025-03-21 22:00:01,671 [fetril.py] => Task 1, Epoch 30/50 => Loss 0.288, Train_accy 91.49
2025-03-21 22:00:03,941 [fetril.py] => Task 1, Epoch 31/50 => Loss 0.287, Train_accy 91.58, Test_accy 70.73
2025-03-21 22:00:05,641 [fetril.py] => Task 1, Epoch 32/50 => Loss 0.287, Train_accy 91.56
2025-03-21 22:00:07,233 [fetril.py] => Task 1, Epoch 33/50 => Loss 0.287, Train_accy 91.48
2025-03-21 22:00:08,768 [fetril.py] => Task 1, Epoch 34/50 => Loss 0.286, Train_accy 91.60
2025-03-21 22:00:10,410 [fetril.py] => Task 1, Epoch 35/50 => Loss 0.286, Train_accy 91.59
2025-03-21 22:00:12,556 [fetril.py] => Task 1, Epoch 36/50 => Loss 0.285, Train_accy 91.70, Test_accy 70.78
2025-03-21 22:00:14,127 [fetril.py] => Task 1, Epoch 37/50 => Loss 0.284, Train_accy 91.73
2025-03-21 22:00:15,733 [fetril.py] => Task 1, Epoch 38/50 => Loss 0.284, Train_accy 91.70
2025-03-21 22:00:17,381 [fetril.py] => Task 1, Epoch 39/50 => Loss 0.283, Train_accy 91.73
2025-03-21 22:00:18,883 [fetril.py] => Task 1, Epoch 40/50 => Loss 0.282, Train_accy 91.76
2025-03-21 22:00:20,952 [fetril.py] => Task 1, Epoch 41/50 => Loss 0.282, Train_accy 91.80, Test_accy 70.87
2025-03-21 22:00:22,607 [fetril.py] => Task 1, Epoch 42/50 => Loss 0.282, Train_accy 91.88
2025-03-21 22:00:24,181 [fetril.py] => Task 1, Epoch 43/50 => Loss 0.281, Train_accy 91.80
2025-03-21 22:00:25,747 [fetril.py] => Task 1, Epoch 44/50 => Loss 0.281, Train_accy 91.79
2025-03-21 22:00:27,263 [fetril.py] => Task 1, Epoch 45/50 => Loss 0.281, Train_accy 91.80
2025-03-21 22:00:29,511 [fetril.py] => Task 1, Epoch 46/50 => Loss 0.281, Train_accy 91.79, Test_accy 70.78
2025-03-21 22:00:31,097 [fetril.py] => Task 1, Epoch 47/50 => Loss 0.280, Train_accy 91.88
2025-03-21 22:00:32,735 [fetril.py] => Task 1, Epoch 48/50 => Loss 0.280, Train_accy 91.88
2025-03-21 22:00:34,302 [fetril.py] => Task 1, Epoch 49/50 => Loss 0.280, Train_accy 91.87
2025-03-21 22:00:35,879 [fetril.py] => Task 1, Epoch 50/50 => Loss 0.280, Train_accy 91.89

2025-03-21 22:00:44,061 [fetril.py] => svm train: acc: 91.17
2025-03-21 22:00:44,072 [fetril.py] => svm evaluation: acc_list: [80.42, 71.35]
Evaluating the model
Calling After Task
2025-03-21 22:00:44,683 [trainer.py] => No NME accuracy.
2025-03-21 22:00:44,684 [trainer.py] => CNN: {'total': 70.68, '00-09': 75.7, '10-19': 67.9, '20-29': 74.4, '30-39': 70.7, '40-49': 73.1, '50-59': 62.3, 'old': 72.36, 'new': 62.3}
2025-03-21 22:00:44,684 [trainer.py] => CNN top1 curve: [80.48, 70.68]
2025-03-21 22:00:44,684 [trainer.py] => CNN top5 curve: [96.58, 91.95]

Average Accuracy (CNN): 75.58000000000001
2025-03-21 22:00:44,684 [trainer.py] => Average Accuracy (CNN): 75.58000000000001
2025-03-21 22:00:44,684 [trainer.py] => All params: 468054
2025-03-21 22:00:44,685 [trainer.py] => Trainable params: 3900
Training the model - Task 2
Calling incremental_train
2025-03-21 22:00:44,691 [fetril.py] => Learning on 60-70
2025-03-21 22:00:44,692 [fetril.py] => All params: 468704
2025-03-21 22:00:44,693 [fetril.py] => Trainable params: 4550
network in the _train function IncrementalNet(
  (convnet): CifarResNet(
    (conv_1_3x3): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (stage_1): Sequential(
      (0): ResNetBasicblock(
        (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): ResNetBasicblock(
        (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): ResNetBasicblock(
        (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): ResNetBasicblock(
        (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (4): ResNetBasicblock(
        (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (stage_2): Sequential(
      (0): ResNetBasicblock(
        (conv_a): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): DownsampleA(
          (avg): AvgPool2d(kernel_size=1, stride=2, padding=0)
        )
      )
      (1): ResNetBasicblock(
        (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): ResNetBasicblock(
        (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): ResNetBasicblock(
        (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (4): ResNetBasicblock(
        (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (stage_3): Sequential(
      (0): ResNetBasicblock(
        (conv_a): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): DownsampleA(
          (avg): AvgPool2d(kernel_size=1, stride=2, padding=0)
        )
      )
      (1): ResNetBasicblock(
        (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): ResNetBasicblock(
        (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): ResNetBasicblock(
        (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (4): ResNetBasicblock(
        (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
    (fc): Linear(in_features=64, out_features=10, bias=True)
  )
  (fc): SimpleLinear()
)
2025-03-21 22:01:02,182 [fetril.py] => Task 2, Epoch 1/50 => Loss 0.640, Train_accy 81.89, Test_accy 66.10
2025-03-21 22:01:03,925 [fetril.py] => Task 2, Epoch 2/50 => Loss 0.508, Train_accy 85.05
2025-03-21 22:01:05,832 [fetril.py] => Task 2, Epoch 3/50 => Loss 0.480, Train_accy 85.79
2025-03-21 22:01:07,556 [fetril.py] => Task 2, Epoch 4/50 => Loss 0.469, Train_accy 86.29
2025-03-21 22:01:09,470 [fetril.py] => Task 2, Epoch 5/50 => Loss 0.460, Train_accy 86.47
2025-03-21 22:01:11,857 [fetril.py] => Task 2, Epoch 6/50 => Loss 0.455, Train_accy 86.55, Test_accy 66.60
2025-03-21 22:01:13,648 [fetril.py] => Task 2, Epoch 7/50 => Loss 0.448, Train_accy 86.83
2025-03-21 22:01:15,473 [fetril.py] => Task 2, Epoch 8/50 => Loss 0.446, Train_accy 86.84
2025-03-21 22:01:17,270 [fetril.py] => Task 2, Epoch 9/50 => Loss 0.442, Train_accy 87.02
2025-03-21 22:01:19,109 [fetril.py] => Task 2, Epoch 10/50 => Loss 0.442, Train_accy 86.91
2025-03-21 22:01:21,681 [fetril.py] => Task 2, Epoch 11/50 => Loss 0.439, Train_accy 86.96, Test_accy 66.13
2025-03-21 22:01:23,478 [fetril.py] => Task 2, Epoch 12/50 => Loss 0.437, Train_accy 87.21
2025-03-21 22:01:25,245 [fetril.py] => Task 2, Epoch 13/50 => Loss 0.434, Train_accy 87.10
2025-03-21 22:01:27,042 [fetril.py] => Task 2, Epoch 14/50 => Loss 0.433, Train_accy 87.24
2025-03-21 22:01:28,909 [fetril.py] => Task 2, Epoch 15/50 => Loss 0.432, Train_accy 87.30
2025-03-21 22:01:31,396 [fetril.py] => Task 2, Epoch 16/50 => Loss 0.430, Train_accy 87.44, Test_accy 66.40
2025-03-21 22:01:33,108 [fetril.py] => Task 2, Epoch 17/50 => Loss 0.429, Train_accy 87.50
2025-03-21 22:01:34,858 [fetril.py] => Task 2, Epoch 18/50 => Loss 0.427, Train_accy 87.62
2025-03-21 22:01:36,738 [fetril.py] => Task 2, Epoch 19/50 => Loss 0.427, Train_accy 87.58
2025-03-21 22:01:38,606 [fetril.py] => Task 2, Epoch 20/50 => Loss 0.426, Train_accy 87.60
2025-03-21 22:01:40,997 [fetril.py] => Task 2, Epoch 21/50 => Loss 0.424, Train_accy 87.72, Test_accy 66.19
2025-03-21 22:01:42,938 [fetril.py] => Task 2, Epoch 22/50 => Loss 0.425, Train_accy 87.59
2025-03-21 22:01:44,735 [fetril.py] => Task 2, Epoch 23/50 => Loss 0.422, Train_accy 87.67
2025-03-21 22:01:46,462 [fetril.py] => Task 2, Epoch 24/50 => Loss 0.422, Train_accy 87.75
2025-03-21 22:01:47,419 [fetril.py] => Task 2, Epoch 25/50 => Loss 0.421, Train_accy 87.76
2025-03-21 22:01:49,188 [fetril.py] => Task 2, Epoch 26/50 => Loss 0.421, Train_accy 87.65, Test_accy 66.33
2025-03-21 22:01:51,241 [fetril.py] => Task 2, Epoch 27/50 => Loss 0.418, Train_accy 87.91
2025-03-21 22:01:52,867 [fetril.py] => Task 2, Epoch 28/50 => Loss 0.418, Train_accy 87.90
2025-03-21 22:01:54,769 [fetril.py] => Task 2, Epoch 29/50 => Loss 0.417, Train_accy 88.02
2025-03-21 22:01:56,665 [fetril.py] => Task 2, Epoch 30/50 => Loss 0.415, Train_accy 88.11
2025-03-21 22:01:59,183 [fetril.py] => Task 2, Epoch 31/50 => Loss 0.415, Train_accy 88.01, Test_accy 66.23
2025-03-21 22:02:00,976 [fetril.py] => Task 2, Epoch 32/50 => Loss 0.414, Train_accy 88.16
2025-03-21 22:02:02,774 [fetril.py] => Task 2, Epoch 33/50 => Loss 0.413, Train_accy 88.18
2025-03-21 22:02:04,660 [fetril.py] => Task 2, Epoch 34/50 => Loss 0.412, Train_accy 88.23
2025-03-21 22:02:06,600 [fetril.py] => Task 2, Epoch 35/50 => Loss 0.411, Train_accy 88.21
2025-03-21 22:02:09,091 [fetril.py] => Task 2, Epoch 36/50 => Loss 0.411, Train_accy 88.28, Test_accy 66.57
2025-03-21 22:02:10,942 [fetril.py] => Task 2, Epoch 37/50 => Loss 0.411, Train_accy 88.37
2025-03-21 22:02:12,772 [fetril.py] => Task 2, Epoch 38/50 => Loss 0.409, Train_accy 88.37
2025-03-21 22:02:14,563 [fetril.py] => Task 2, Epoch 39/50 => Loss 0.408, Train_accy 88.34
2025-03-21 22:02:16,273 [fetril.py] => Task 2, Epoch 40/50 => Loss 0.407, Train_accy 88.47
2025-03-21 22:02:18,899 [fetril.py] => Task 2, Epoch 41/50 => Loss 0.407, Train_accy 88.36, Test_accy 66.67
2025-03-21 22:02:20,723 [fetril.py] => Task 2, Epoch 42/50 => Loss 0.407, Train_accy 88.57
2025-03-21 22:02:22,558 [fetril.py] => Task 2, Epoch 43/50 => Loss 0.406, Train_accy 88.52
2025-03-21 22:02:24,245 [fetril.py] => Task 2, Epoch 44/50 => Loss 0.405, Train_accy 88.57
2025-03-21 22:02:26,107 [fetril.py] => Task 2, Epoch 45/50 => Loss 0.405, Train_accy 88.55
2025-03-21 22:02:28,609 [fetril.py] => Task 2, Epoch 46/50 => Loss 0.405, Train_accy 88.61, Test_accy 66.64
2025-03-21 22:02:30,487 [fetril.py] => Task 2, Epoch 47/50 => Loss 0.405, Train_accy 88.57
2025-03-21 22:02:32,223 [fetril.py] => Task 2, Epoch 48/50 => Loss 0.404, Train_accy 88.60
2025-03-21 22:02:34,065 [fetril.py] => Task 2, Epoch 49/50 => Loss 0.404, Train_accy 88.61
2025-03-21 22:02:35,961 [fetril.py] => Task 2, Epoch 50/50 => Loss 0.404, Train_accy 88.63

2025-03-21 22:02:47,676 [fetril.py] => svm train: acc: 87.31
2025-03-21 22:02:47,703 [fetril.py] => svm evaluation: acc_list: [80.42, 71.35, 66.41]
Evaluating the model
Calling After Task
2025-03-21 22:02:48,414 [trainer.py] => No NME accuracy.
2025-03-21 22:02:48,414 [trainer.py] => CNN: {'total': 66.74, '00-09': 76.2, '10-19': 63.9, '20-29': 73.7, '30-39': 68.4, '40-49': 70.8, '50-59': 51.3, '60-69': 62.9, 'old': 67.38, 'new': 62.9}
2025-03-21 22:02:48,414 [trainer.py] => CNN top1 curve: [80.48, 70.68, 66.74]
2025-03-21 22:02:48,414 [trainer.py] => CNN top5 curve: [96.58, 91.95, 89.47]

Average Accuracy (CNN): 72.63333333333334
2025-03-21 22:02:48,414 [trainer.py] => Average Accuracy (CNN): 72.63333333333334
2025-03-21 22:02:48,414 [trainer.py] => All params: 468704
2025-03-21 22:02:48,415 [trainer.py] => Trainable params: 4550
Training the model - Task 3
Calling incremental_train
2025-03-21 22:02:48,422 [fetril.py] => Learning on 70-80
2025-03-21 22:02:48,423 [fetril.py] => All params: 469354
2025-03-21 22:02:48,423 [fetril.py] => Trainable params: 5200
network in the _train function IncrementalNet(
  (convnet): CifarResNet(
    (conv_1_3x3): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (stage_1): Sequential(
      (0): ResNetBasicblock(
        (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): ResNetBasicblock(
        (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): ResNetBasicblock(
        (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): ResNetBasicblock(
        (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (4): ResNetBasicblock(
        (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (stage_2): Sequential(
      (0): ResNetBasicblock(
        (conv_a): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): DownsampleA(
          (avg): AvgPool2d(kernel_size=1, stride=2, padding=0)
        )
      )
      (1): ResNetBasicblock(
        (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): ResNetBasicblock(
        (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): ResNetBasicblock(
        (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (4): ResNetBasicblock(
        (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (stage_3): Sequential(
      (0): ResNetBasicblock(
        (conv_a): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): DownsampleA(
          (avg): AvgPool2d(kernel_size=1, stride=2, padding=0)
        )
      )
      (1): ResNetBasicblock(
        (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): ResNetBasicblock(
        (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): ResNetBasicblock(
        (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (4): ResNetBasicblock(
        (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
    (fc): Linear(in_features=64, out_features=10, bias=True)
  )
  (fc): SimpleLinear()
)
2025-03-21 22:03:07,403 [fetril.py] => Task 3, Epoch 1/50 => Loss 0.818, Train_accy 76.50, Test_accy 62.05
2025-03-21 22:03:09,419 [fetril.py] => Task 3, Epoch 2/50 => Loss 0.678, Train_accy 79.35
2025-03-21 22:03:11,452 [fetril.py] => Task 3, Epoch 3/50 => Loss 0.651, Train_accy 80.28
2025-03-21 22:03:13,583 [fetril.py] => Task 3, Epoch 4/50 => Loss 0.638, Train_accy 80.89
2025-03-21 22:03:15,529 [fetril.py] => Task 3, Epoch 5/50 => Loss 0.628, Train_accy 81.16
2025-03-21 22:03:18,312 [fetril.py] => Task 3, Epoch 6/50 => Loss 0.622, Train_accy 81.35, Test_accy 61.88
2025-03-21 22:03:20,388 [fetril.py] => Task 3, Epoch 7/50 => Loss 0.613, Train_accy 81.69
2025-03-21 22:03:22,336 [fetril.py] => Task 3, Epoch 8/50 => Loss 0.611, Train_accy 81.79
2025-03-21 22:03:24,239 [fetril.py] => Task 3, Epoch 9/50 => Loss 0.608, Train_accy 81.88
2025-03-21 22:03:26,236 [fetril.py] => Task 3, Epoch 10/50 => Loss 0.607, Train_accy 81.86
2025-03-21 22:03:29,012 [fetril.py] => Task 3, Epoch 11/50 => Loss 0.602, Train_accy 82.08, Test_accy 62.31
2025-03-21 22:03:31,021 [fetril.py] => Task 3, Epoch 12/50 => Loss 0.601, Train_accy 82.15
2025-03-21 22:03:32,997 [fetril.py] => Task 3, Epoch 13/50 => Loss 0.599, Train_accy 82.12
2025-03-21 22:03:35,063 [fetril.py] => Task 3, Epoch 14/50 => Loss 0.597, Train_accy 82.11
2025-03-21 22:03:37,103 [fetril.py] => Task 3, Epoch 15/50 => Loss 0.595, Train_accy 82.35
2025-03-21 22:03:39,846 [fetril.py] => Task 3, Epoch 16/50 => Loss 0.595, Train_accy 82.23, Test_accy 62.20
2025-03-21 22:03:41,840 [fetril.py] => Task 3, Epoch 17/50 => Loss 0.591, Train_accy 82.54
2025-03-21 22:03:43,902 [fetril.py] => Task 3, Epoch 18/50 => Loss 0.590, Train_accy 82.60
2025-03-21 22:03:45,906 [fetril.py] => Task 3, Epoch 19/50 => Loss 0.588, Train_accy 82.62
2025-03-21 22:03:47,952 [fetril.py] => Task 3, Epoch 20/50 => Loss 0.588, Train_accy 82.76
2025-03-21 22:03:50,712 [fetril.py] => Task 3, Epoch 21/50 => Loss 0.586, Train_accy 82.66, Test_accy 62.20
2025-03-21 22:03:52,624 [fetril.py] => Task 3, Epoch 22/50 => Loss 0.585, Train_accy 82.81
2025-03-21 22:03:54,686 [fetril.py] => Task 3, Epoch 23/50 => Loss 0.583, Train_accy 82.74
2025-03-21 22:03:56,799 [fetril.py] => Task 3, Epoch 24/50 => Loss 0.582, Train_accy 82.74
2025-03-21 22:03:58,787 [fetril.py] => Task 3, Epoch 25/50 => Loss 0.580, Train_accy 83.01
2025-03-21 22:04:01,531 [fetril.py] => Task 3, Epoch 26/50 => Loss 0.578, Train_accy 83.09, Test_accy 62.45
2025-03-21 22:04:03,616 [fetril.py] => Task 3, Epoch 27/50 => Loss 0.578, Train_accy 83.04
2025-03-21 22:04:05,667 [fetril.py] => Task 3, Epoch 28/50 => Loss 0.576, Train_accy 83.12
2025-03-21 22:04:07,599 [fetril.py] => Task 3, Epoch 29/50 => Loss 0.574, Train_accy 83.18
2025-03-21 22:04:09,653 [fetril.py] => Task 3, Epoch 30/50 => Loss 0.574, Train_accy 83.20
2025-03-21 22:04:12,437 [fetril.py] => Task 3, Epoch 31/50 => Loss 0.572, Train_accy 83.35, Test_accy 62.36
2025-03-21 22:04:14,448 [fetril.py] => Task 3, Epoch 32/50 => Loss 0.572, Train_accy 83.44
2025-03-21 22:04:16,470 [fetril.py] => Task 3, Epoch 33/50 => Loss 0.571, Train_accy 83.44
2025-03-21 22:04:18,557 [fetril.py] => Task 3, Epoch 34/50 => Loss 0.569, Train_accy 83.70
2025-03-21 22:04:20,661 [fetril.py] => Task 3, Epoch 35/50 => Loss 0.568, Train_accy 83.64
2025-03-21 22:04:23,374 [fetril.py] => Task 3, Epoch 36/50 => Loss 0.567, Train_accy 83.66, Test_accy 62.48
2025-03-21 22:04:25,539 [fetril.py] => Task 3, Epoch 37/50 => Loss 0.566, Train_accy 83.54
2025-03-21 22:04:27,524 [fetril.py] => Task 3, Epoch 38/50 => Loss 0.566, Train_accy 83.85
2025-03-21 22:04:29,462 [fetril.py] => Task 3, Epoch 39/50 => Loss 0.565, Train_accy 83.77
2025-03-21 22:04:31,472 [fetril.py] => Task 3, Epoch 40/50 => Loss 0.563, Train_accy 83.88
2025-03-21 22:04:34,278 [fetril.py] => Task 3, Epoch 41/50 => Loss 0.563, Train_accy 83.94, Test_accy 62.59
2025-03-21 22:04:36,410 [fetril.py] => Task 3, Epoch 42/50 => Loss 0.562, Train_accy 83.98
2025-03-21 22:04:38,419 [fetril.py] => Task 3, Epoch 43/50 => Loss 0.561, Train_accy 83.99
2025-03-21 22:04:40,441 [fetril.py] => Task 3, Epoch 44/50 => Loss 0.561, Train_accy 84.02
2025-03-21 22:04:42,492 [fetril.py] => Task 3, Epoch 45/50 => Loss 0.560, Train_accy 84.07
2025-03-21 22:04:45,217 [fetril.py] => Task 3, Epoch 46/50 => Loss 0.560, Train_accy 84.08, Test_accy 62.90
2025-03-21 22:04:47,259 [fetril.py] => Task 3, Epoch 47/50 => Loss 0.559, Train_accy 84.14
2025-03-21 22:04:49,255 [fetril.py] => Task 3, Epoch 48/50 => Loss 0.559, Train_accy 84.14
2025-03-21 22:04:51,283 [fetril.py] => Task 3, Epoch 49/50 => Loss 0.558, Train_accy 84.16
2025-03-21 22:04:53,215 [fetril.py] => Task 3, Epoch 50/50 => Loss 0.558, Train_accy 84.16

2025-03-21 22:05:09,664 [fetril.py] => svm train: acc: 82.47
2025-03-21 22:05:09,683 [fetril.py] => svm evaluation: acc_list: [80.42, 71.35, 66.41, 62.29]
Evaluating the model
Calling After Task
2025-03-21 22:05:10,399 [trainer.py] => No NME accuracy.
2025-03-21 22:05:10,399 [trainer.py] => CNN: {'total': 62.82, '00-09': 74.1, '10-19': 62.8, '20-29': 73.9, '30-39': 66.7, '40-49': 69.7, '50-59': 45.6, '60-69': 53.9, '70-79': 55.9, 'old': 63.81, 'new': 55.9}
2025-03-21 22:05:10,399 [trainer.py] => CNN top1 curve: [80.48, 70.68, 66.74, 62.82]
2025-03-21 22:05:10,399 [trainer.py] => CNN top5 curve: [96.58, 91.95, 89.47, 87.48]

Average Accuracy (CNN): 70.18
2025-03-21 22:05:10,399 [trainer.py] => Average Accuracy (CNN): 70.18
2025-03-21 22:05:10,400 [trainer.py] => All params: 469354
2025-03-21 22:05:10,400 [trainer.py] => Trainable params: 5200
Training the model - Task 4
Calling incremental_train
2025-03-21 22:05:10,407 [fetril.py] => Learning on 80-90
2025-03-21 22:05:10,408 [fetril.py] => All params: 470004
2025-03-21 22:05:10,409 [fetril.py] => Trainable params: 5850
network in the _train function IncrementalNet(
  (convnet): CifarResNet(
    (conv_1_3x3): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (stage_1): Sequential(
      (0): ResNetBasicblock(
        (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): ResNetBasicblock(
        (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): ResNetBasicblock(
        (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): ResNetBasicblock(
        (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (4): ResNetBasicblock(
        (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (stage_2): Sequential(
      (0): ResNetBasicblock(
        (conv_a): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): DownsampleA(
          (avg): AvgPool2d(kernel_size=1, stride=2, padding=0)
        )
      )
      (1): ResNetBasicblock(
        (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): ResNetBasicblock(
        (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): ResNetBasicblock(
        (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (4): ResNetBasicblock(
        (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (stage_3): Sequential(
      (0): ResNetBasicblock(
        (conv_a): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): DownsampleA(
          (avg): AvgPool2d(kernel_size=1, stride=2, padding=0)
        )
      )
      (1): ResNetBasicblock(
        (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): ResNetBasicblock(
        (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): ResNetBasicblock(
        (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (4): ResNetBasicblock(
        (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
    (fc): Linear(in_features=64, out_features=10, bias=True)
  )
  (fc): SimpleLinear()
)
2025-03-21 22:05:31,634 [fetril.py] => Task 4, Epoch 1/50 => Loss 0.996, Train_accy 71.76, Test_accy 58.40
2025-03-21 22:05:33,800 [fetril.py] => Task 4, Epoch 2/50 => Loss 0.841, Train_accy 75.34
2025-03-21 22:05:36,075 [fetril.py] => Task 4, Epoch 3/50 => Loss 0.805, Train_accy 76.46
2025-03-21 22:05:38,371 [fetril.py] => Task 4, Epoch 4/50 => Loss 0.791, Train_accy 77.07
2025-03-21 22:05:40,675 [fetril.py] => Task 4, Epoch 5/50 => Loss 0.779, Train_accy 77.23
2025-03-21 22:05:43,778 [fetril.py] => Task 4, Epoch 6/50 => Loss 0.770, Train_accy 77.56, Test_accy 58.81
2025-03-21 22:05:46,110 [fetril.py] => Task 4, Epoch 7/50 => Loss 0.767, Train_accy 77.61
2025-03-21 22:05:48,395 [fetril.py] => Task 4, Epoch 8/50 => Loss 0.763, Train_accy 77.77
2025-03-21 22:05:50,579 [fetril.py] => Task 4, Epoch 9/50 => Loss 0.760, Train_accy 77.81
2025-03-21 22:05:52,817 [fetril.py] => Task 4, Epoch 10/50 => Loss 0.757, Train_accy 78.15
2025-03-21 22:05:55,856 [fetril.py] => Task 4, Epoch 11/50 => Loss 0.752, Train_accy 78.07, Test_accy 58.70
2025-03-21 22:05:58,164 [fetril.py] => Task 4, Epoch 12/50 => Loss 0.749, Train_accy 78.24
2025-03-21 22:06:00,517 [fetril.py] => Task 4, Epoch 13/50 => Loss 0.750, Train_accy 78.36
2025-03-21 22:06:02,765 [fetril.py] => Task 4, Epoch 14/50 => Loss 0.747, Train_accy 78.35
2025-03-21 22:06:04,960 [fetril.py] => Task 4, Epoch 15/50 => Loss 0.745, Train_accy 78.65
2025-03-21 22:06:07,992 [fetril.py] => Task 4, Epoch 16/50 => Loss 0.742, Train_accy 78.50, Test_accy 58.53
2025-03-21 22:06:10,300 [fetril.py] => Task 4, Epoch 17/50 => Loss 0.742, Train_accy 78.52
2025-03-21 22:06:12,555 [fetril.py] => Task 4, Epoch 18/50 => Loss 0.739, Train_accy 78.68
2025-03-21 22:06:14,849 [fetril.py] => Task 4, Epoch 19/50 => Loss 0.738, Train_accy 78.73
2025-03-21 22:06:17,179 [fetril.py] => Task 4, Epoch 20/50 => Loss 0.735, Train_accy 78.90
2025-03-21 22:06:20,081 [fetril.py] => Task 4, Epoch 21/50 => Loss 0.735, Train_accy 78.96, Test_accy 58.89
2025-03-21 22:06:22,315 [fetril.py] => Task 4, Epoch 22/50 => Loss 0.734, Train_accy 78.91
2025-03-21 22:06:24,523 [fetril.py] => Task 4, Epoch 23/50 => Loss 0.732, Train_accy 79.14
2025-03-21 22:06:26,746 [fetril.py] => Task 4, Epoch 24/50 => Loss 0.731, Train_accy 79.03
2025-03-21 22:06:29,049 [fetril.py] => Task 4, Epoch 25/50 => Loss 0.729, Train_accy 79.26
2025-03-21 22:06:32,208 [fetril.py] => Task 4, Epoch 26/50 => Loss 0.726, Train_accy 79.18, Test_accy 59.08
2025-03-21 22:06:34,405 [fetril.py] => Task 4, Epoch 27/50 => Loss 0.725, Train_accy 79.25
2025-03-21 22:06:36,718 [fetril.py] => Task 4, Epoch 28/50 => Loss 0.723, Train_accy 79.32
2025-03-21 22:06:39,024 [fetril.py] => Task 4, Epoch 29/50 => Loss 0.723, Train_accy 79.37
2025-03-21 22:06:41,234 [fetril.py] => Task 4, Epoch 30/50 => Loss 0.720, Train_accy 79.58
2025-03-21 22:06:44,354 [fetril.py] => Task 4, Epoch 31/50 => Loss 0.719, Train_accy 79.57, Test_accy 59.03
2025-03-21 22:06:46,612 [fetril.py] => Task 4, Epoch 32/50 => Loss 0.719, Train_accy 79.65
2025-03-21 22:06:48,826 [fetril.py] => Task 4, Epoch 33/50 => Loss 0.716, Train_accy 79.75
2025-03-21 22:06:51,131 [fetril.py] => Task 4, Epoch 34/50 => Loss 0.715, Train_accy 79.84
2025-03-21 22:06:53,401 [fetril.py] => Task 4, Epoch 35/50 => Loss 0.713, Train_accy 79.84
2025-03-21 22:06:56,402 [fetril.py] => Task 4, Epoch 36/50 => Loss 0.713, Train_accy 80.08, Test_accy 59.01
2025-03-21 22:06:58,696 [fetril.py] => Task 4, Epoch 37/50 => Loss 0.712, Train_accy 79.97
2025-03-21 22:07:00,979 [fetril.py] => Task 4, Epoch 38/50 => Loss 0.710, Train_accy 80.07
2025-03-21 22:07:03,194 [fetril.py] => Task 4, Epoch 39/50 => Loss 0.709, Train_accy 80.09
2025-03-21 22:07:05,501 [fetril.py] => Task 4, Epoch 40/50 => Loss 0.708, Train_accy 80.23
2025-03-21 22:07:08,608 [fetril.py] => Task 4, Epoch 41/50 => Loss 0.707, Train_accy 80.18, Test_accy 59.27
2025-03-21 22:07:10,763 [fetril.py] => Task 4, Epoch 42/50 => Loss 0.706, Train_accy 80.30
2025-03-21 22:07:13,099 [fetril.py] => Task 4, Epoch 43/50 => Loss 0.705, Train_accy 80.42
2025-03-21 22:07:15,328 [fetril.py] => Task 4, Epoch 44/50 => Loss 0.704, Train_accy 80.34
2025-03-21 22:07:17,645 [fetril.py] => Task 4, Epoch 45/50 => Loss 0.704, Train_accy 80.41
2025-03-21 22:07:20,752 [fetril.py] => Task 4, Epoch 46/50 => Loss 0.704, Train_accy 80.41, Test_accy 59.42
2025-03-21 22:07:23,046 [fetril.py] => Task 4, Epoch 47/50 => Loss 0.703, Train_accy 80.51
2025-03-21 22:07:25,324 [fetril.py] => Task 4, Epoch 48/50 => Loss 0.703, Train_accy 80.50
2025-03-21 22:07:27,628 [fetril.py] => Task 4, Epoch 49/50 => Loss 0.703, Train_accy 80.51
2025-03-21 22:07:29,901 [fetril.py] => Task 4, Epoch 50/50 => Loss 0.702, Train_accy 80.53

2025-03-21 22:07:51,427 [fetril.py] => svm train: acc: 78.55
2025-03-21 22:07:51,442 [fetril.py] => svm evaluation: acc_list: [80.42, 71.35, 66.41, 62.29, 58.67]
Evaluating the model
Calling After Task
2025-03-21 22:07:52,253 [trainer.py] => No NME accuracy.
2025-03-21 22:07:52,253 [trainer.py] => CNN: {'total': 59.28, '00-09': 72.6, '10-19': 61.3, '20-29': 71.1, '30-39': 65.4, '40-49': 67.4, '50-59': 41.0, '60-69': 51.5, '70-79': 49.4, '80-89': 53.8, 'old': 59.96, 'new': 53.8}
2025-03-21 22:07:52,254 [trainer.py] => CNN top1 curve: [80.48, 70.68, 66.74, 62.82, 59.28]
2025-03-21 22:07:52,254 [trainer.py] => CNN top5 curve: [96.58, 91.95, 89.47, 87.48, 85.5]

Average Accuracy (CNN): 68.0
2025-03-21 22:07:52,254 [trainer.py] => Average Accuracy (CNN): 68.0
2025-03-21 22:07:52,254 [trainer.py] => All params: 470004
2025-03-21 22:07:52,255 [trainer.py] => Trainable params: 5850
Training the model - Task 5
Calling incremental_train
2025-03-21 22:07:52,262 [fetril.py] => Learning on 90-100
2025-03-21 22:07:52,263 [fetril.py] => All params: 470654
2025-03-21 22:07:52,263 [fetril.py] => Trainable params: 6500
network in the _train function IncrementalNet(
  (convnet): CifarResNet(
    (conv_1_3x3): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (stage_1): Sequential(
      (0): ResNetBasicblock(
        (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): ResNetBasicblock(
        (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): ResNetBasicblock(
        (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): ResNetBasicblock(
        (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (4): ResNetBasicblock(
        (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (stage_2): Sequential(
      (0): ResNetBasicblock(
        (conv_a): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): DownsampleA(
          (avg): AvgPool2d(kernel_size=1, stride=2, padding=0)
        )
      )
      (1): ResNetBasicblock(
        (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): ResNetBasicblock(
        (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): ResNetBasicblock(
        (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (4): ResNetBasicblock(
        (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (stage_3): Sequential(
      (0): ResNetBasicblock(
        (conv_a): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): DownsampleA(
          (avg): AvgPool2d(kernel_size=1, stride=2, padding=0)
        )
      )
      (1): ResNetBasicblock(
        (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): ResNetBasicblock(
        (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): ResNetBasicblock(
        (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (4): ResNetBasicblock(
        (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
    (fc): Linear(in_features=64, out_features=10, bias=True)
  )
  (fc): SimpleLinear()
)
2025-03-21 22:08:14,243 [fetril.py] => Task 5, Epoch 1/50 => Loss 1.085, Train_accy 69.59, Test_accy 55.99
2025-03-21 22:08:16,686 [fetril.py] => Task 5, Epoch 2/50 => Loss 0.927, Train_accy 73.24
2025-03-21 22:08:19,061 [fetril.py] => Task 5, Epoch 3/50 => Loss 0.885, Train_accy 74.49
2025-03-21 22:08:21,646 [fetril.py] => Task 5, Epoch 4/50 => Loss 0.867, Train_accy 75.09
2025-03-21 22:08:24,084 [fetril.py] => Task 5, Epoch 5/50 => Loss 0.855, Train_accy 75.47
2025-03-21 22:08:27,437 [fetril.py] => Task 5, Epoch 6/50 => Loss 0.845, Train_accy 75.88, Test_accy 55.56
2025-03-21 22:08:29,924 [fetril.py] => Task 5, Epoch 7/50 => Loss 0.840, Train_accy 75.93
2025-03-21 22:08:32,403 [fetril.py] => Task 5, Epoch 8/50 => Loss 0.837, Train_accy 76.06
2025-03-21 22:08:34,863 [fetril.py] => Task 5, Epoch 9/50 => Loss 0.831, Train_accy 76.38
2025-03-21 22:08:37,333 [fetril.py] => Task 5, Epoch 10/50 => Loss 0.828, Train_accy 76.33
2025-03-21 22:08:40,701 [fetril.py] => Task 5, Epoch 11/50 => Loss 0.827, Train_accy 76.31, Test_accy 56.19
2025-03-21 22:08:43,231 [fetril.py] => Task 5, Epoch 12/50 => Loss 0.822, Train_accy 76.51
2025-03-21 22:08:45,534 [fetril.py] => Task 5, Epoch 13/50 => Loss 0.821, Train_accy 76.63
2025-03-21 22:08:48,054 [fetril.py] => Task 5, Epoch 14/50 => Loss 0.819, Train_accy 76.63
2025-03-21 22:08:50,575 [fetril.py] => Task 5, Epoch 15/50 => Loss 0.817, Train_accy 76.85
2025-03-21 22:08:53,784 [fetril.py] => Task 5, Epoch 16/50 => Loss 0.814, Train_accy 76.93, Test_accy 55.84
2025-03-21 22:08:56,323 [fetril.py] => Task 5, Epoch 17/50 => Loss 0.811, Train_accy 76.99
2025-03-21 22:08:58,811 [fetril.py] => Task 5, Epoch 18/50 => Loss 0.809, Train_accy 77.16
2025-03-21 22:09:01,193 [fetril.py] => Task 5, Epoch 19/50 => Loss 0.807, Train_accy 77.22
2025-03-21 22:09:03,625 [fetril.py] => Task 5, Epoch 20/50 => Loss 0.806, Train_accy 77.18
2025-03-21 22:09:06,967 [fetril.py] => Task 5, Epoch 21/50 => Loss 0.806, Train_accy 77.26, Test_accy 55.86
2025-03-21 22:09:09,406 [fetril.py] => Task 5, Epoch 22/50 => Loss 0.802, Train_accy 77.27
2025-03-21 22:09:11,873 [fetril.py] => Task 5, Epoch 23/50 => Loss 0.800, Train_accy 77.46
2025-03-21 22:09:14,305 [fetril.py] => Task 5, Epoch 24/50 => Loss 0.799, Train_accy 77.48
2025-03-21 22:09:16,782 [fetril.py] => Task 5, Epoch 25/50 => Loss 0.797, Train_accy 77.73
2025-03-21 22:09:20,269 [fetril.py] => Task 5, Epoch 26/50 => Loss 0.795, Train_accy 77.67, Test_accy 55.99
2025-03-21 22:09:22,545 [fetril.py] => Task 5, Epoch 27/50 => Loss 0.794, Train_accy 77.70
2025-03-21 22:09:25,014 [fetril.py] => Task 5, Epoch 28/50 => Loss 0.791, Train_accy 77.86
2025-03-21 22:09:27,464 [fetril.py] => Task 5, Epoch 29/50 => Loss 0.792, Train_accy 77.96
2025-03-21 22:09:29,860 [fetril.py] => Task 5, Epoch 30/50 => Loss 0.789, Train_accy 77.95
2025-03-21 22:09:33,234 [fetril.py] => Task 5, Epoch 31/50 => Loss 0.787, Train_accy 78.09, Test_accy 56.30
2025-03-21 22:09:35,671 [fetril.py] => Task 5, Epoch 32/50 => Loss 0.785, Train_accy 78.23
2025-03-21 22:09:38,167 [fetril.py] => Task 5, Epoch 33/50 => Loss 0.784, Train_accy 78.24
2025-03-21 22:09:40,613 [fetril.py] => Task 5, Epoch 34/50 => Loss 0.783, Train_accy 78.24
2025-03-21 22:09:43,158 [fetril.py] => Task 5, Epoch 35/50 => Loss 0.781, Train_accy 78.37
2025-03-21 22:09:46,476 [fetril.py] => Task 5, Epoch 36/50 => Loss 0.780, Train_accy 78.44, Test_accy 56.37
2025-03-21 22:09:48,997 [fetril.py] => Task 5, Epoch 37/50 => Loss 0.778, Train_accy 78.45
2025-03-21 22:09:51,424 [fetril.py] => Task 5, Epoch 38/50 => Loss 0.777, Train_accy 78.58
2025-03-21 22:09:53,986 [fetril.py] => Task 5, Epoch 39/50 => Loss 0.776, Train_accy 78.49
2025-03-21 22:09:56,539 [fetril.py] => Task 5, Epoch 40/50 => Loss 0.775, Train_accy 78.66
2025-03-21 22:09:59,920 [fetril.py] => Task 5, Epoch 41/50 => Loss 0.774, Train_accy 78.75, Test_accy 56.40
2025-03-21 22:10:02,439 [fetril.py] => Task 5, Epoch 42/50 => Loss 0.773, Train_accy 78.70
2025-03-21 22:10:04,889 [fetril.py] => Task 5, Epoch 43/50 => Loss 0.772, Train_accy 78.70
2025-03-21 22:10:07,314 [fetril.py] => Task 5, Epoch 44/50 => Loss 0.771, Train_accy 78.83
2025-03-21 22:10:09,804 [fetril.py] => Task 5, Epoch 45/50 => Loss 0.771, Train_accy 78.83
2025-03-21 22:10:13,139 [fetril.py] => Task 5, Epoch 46/50 => Loss 0.770, Train_accy 78.84, Test_accy 56.61
2025-03-21 22:10:15,537 [fetril.py] => Task 5, Epoch 47/50 => Loss 0.770, Train_accy 78.86
2025-03-21 22:10:17,986 [fetril.py] => Task 5, Epoch 48/50 => Loss 0.769, Train_accy 78.87
2025-03-21 22:10:20,375 [fetril.py] => Task 5, Epoch 49/50 => Loss 0.768, Train_accy 78.90
2025-03-21 22:10:22,847 [fetril.py] => Task 5, Epoch 50/50 => Loss 0.768, Train_accy 78.89

2025-03-21 22:10:51,120 [fetril.py] => svm train: acc: 77.36
2025-03-21 22:10:51,150 [fetril.py] => svm evaluation: acc_list: [80.42, 71.35, 66.41, 62.29, 58.67, 56.05]
Evaluating the model
Calling After Task
2025-03-21 22:10:52,047 [trainer.py] => No NME accuracy.
2025-03-21 22:10:52,048 [trainer.py] => CNN: {'total': 56.58, '00-09': 70.9, '10-19': 60.3, '20-29': 70.0, '30-39': 65.0, '40-49': 65.9, '50-59': 38.3, '60-69': 47.2, '70-79': 44.5, '80-89': 48.0, '90-99': 55.7, 'old': 56.68, 'new': 55.7}
2025-03-21 22:10:52,048 [trainer.py] => CNN top1 curve: [80.48, 70.68, 66.74, 62.82, 59.28, 56.58]
2025-03-21 22:10:52,048 [trainer.py] => CNN top5 curve: [96.58, 91.95, 89.47, 87.48, 85.5, 84.13]

Average Accuracy (CNN): 66.09666666666666
2025-03-21 22:10:52,048 [trainer.py] => Average Accuracy (CNN): 66.09666666666666
